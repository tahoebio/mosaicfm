integrations:
- integration_type: git_repo
  git_repo: vevotx/mosaicfm
  git_branch: add-evo2-embeddings-for-genes-in-vocabulary
  pip_install: -e . --no-deps
  ssh_clone: true  # Should be true if using a private repo

command: |
  cd mosaicfm/scripts
  composer train.py /mnt/config/parameters.yaml
image: vevotx/mosaicfm:1.0.0
env_variables:
  MOSAICML_PLATFORM: False
name: mosaicfm-70m-evo2

compute:
  gpus: 16 # Number of GPUs to use
  cluster: r15z1p1
scheduling:
  max_duration: 0.5 # Maximum duration of the run in hours

parameters:
  global_seed: 777
  seed: ${global_seed}
  device_train_batch_size: 400
  global_train_batch_size: 4800
  device_eval_batch_size: 400
  device_train_microbatch_size: 'auto'
  vocabulary:
    remote: "s3://vevo-ml-datasets/mosaicfm_v2/datasets/vevo_v2_vocab.json"
    local: "vocab.json"
  model:
    name: vevo_scgpt_with_evo2
    d_model: 512
    n_layers: 12
    init_device: cpu
    expansion_ratio: 4
    standard_scale_outputs: False
    transformer_activation: relu
    n_heads: 8
    norm_scheme: "pre"
    use_generative_training: True
    use_cell_conditioned_generation: False
    use_glu: False
    cell_emb_style: cls
    attn_config:
      attn_impl: flash
      attn_type: "grouped_query_attention"
      use_attn_mask: False
      kv_nheads: 8
      attn_pdrop: 0.0
    norm_config:
      norm_type: "layernorm"
      eps: 1.0e-5
    expression_encoder:
      input_emb_style: "continuous"
      dropout: 0.1
      max_value: 512
      activation: relu
      use_norm: True
    gene_encoder:
      use_norm: True
      gene_embedding:
        trainable: true  # Whether to use primary trainable embeddings
        embeddings:
          evo2:
            remote: "s3://vevo-ml-datasets/evo2-embeddings/gene_evo27b_250504_embeddings_vocab_aligned.pt"
            local: "gene_evo27b_250504_embeddings_vocab_aligned.pt"
            use_norm: true  # Whether to normalize this embedding after projection
            fix_embedding: true  # True to keep Evo2 embeddings fixed
    mvc:
      arch_style: "inner product"
      query_activation: "sigmoid"
      scaled_dot_product: True
    expression_decoder:
      n_outputs: 1
      n_layers: 1
      activation: "leaky_relu"
  collator:
    do_padding: True
    pad_value: -2
    do_mlm: True
    do_binning: True
    mlm_probability: 0.5
    mask_value: -1
    max_length: 1024
    sampling: True
    data_style: "both"
    num_bins: 51
    right_binning: False
    use_junk_tokens: False
  train_loader:
    dataset:
      streams:
        tahoe:
          remote: "s3://vevo-ml-datasets/mosaicfm_v2/datasets/tahoe_100m_MDS_v2/train/"
          local: "mds-data-folder/tahoe/train"
        cellxgene:
          remote: "s3://vevo-ml-datasets/mosaicfm_v2/datasets/cellxgene_2025_01_21_merged_MDS/train/"
          local: "mds-data-folder/cellxgene/train"
      download_timeout: 300
      allow_unsafe_types: True
      shuffle: True
      shuffle_seed: ${global_seed}
      num_canonical_nodes: 2
    drop_last: False
    num_workers: 8
    pin_memory: True
    prefetch_factor: 48
    persistent_workers: True
  valid_loader:
    dataset:
      streams:
        tahoe:
          remote: "s3://vevo-ml-datasets/mosaicfm_v2/datasets/tahoe_100m_MDS_v2/valid/"
          local: "mds-data-folder/tahoe/val"
        cellxgene:
          remote: "s3://vevo-ml-datasets/mosaicfm_v2/datasets/cellxgene_2025_01_21_merged_MDS/valid/"
          local: "mds-data-folder/cellxgene/val"
      download_timeout: 300
      allow_unsafe_types: True
      shuffle: False
      shuffle_seed: ${global_seed}
      num_canonical_nodes: 2
    drop_last: False
    num_workers: 8
    pin_memory: True
    prefetch_factor: 48
    persistent_workers: True
  optimizer:
    name: decoupled_adamw
    lr: 3.0e-4
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 1.0e-05
  scheduler:
    name: cosine_with_warmup
    t_warmup: "0.05dur"
    t_max: "1dur"
    alpha_f: 0.1
  algorithms:
    gradient_clipping:
      clipping_type: norm
      clipping_threshold: 1.0
    low_precision_layernorm: {}
  precision: amp_bf16
  eval_interval: "5000ba"
  eval_subset_num_batches: 100
  max_duration: "16700ba" # 16700 * 4800 = 80.16 M cells
  fsdp_config:
    sharding_strategy: FULL_SHARD
    mixed_precision: DEFAULT
    activation_checkpointing: false
    activation_checkpointing_reentrant: false
    activation_cpu_offload: false
    limit_all_gathers: true
    verbose: true
  callbacks:
    speed_monitor:
      window_size: 20
    lr_monitor: {}
    memory_monitor: {}
    runtime_estimator: {}
  loggers:
    wandb:
      project: mosaicfm-dev-evo2
      group: evo2-embedding
      name: mosaicfm-70m-evo2-per-dim-mix
      log_artifacts: False
  save_folder: "s3://vevo-ml-datasets/vevo-scgpt/models/{run_name}"
  save_interval: "2000ba"
  autoresume: True